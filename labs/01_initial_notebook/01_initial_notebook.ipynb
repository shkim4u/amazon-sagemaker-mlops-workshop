{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "878d65c2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# MLOps workshop with Amazon SageMaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5074715-59a1-4b2c-a67d-c01217b31106",
   "metadata": {},
   "source": [
    "## Module 01: Transform the data and train a model inside a Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a3707f",
   "metadata": {},
   "source": [
    "In this workshop we will demonstrate a journey to cloud-native machine learning starting from a more traditional approach to model development and training directly in Jupyter notebooks to remote managed data transformations and training with Amazon SageMaker to fully automated pipelines with SageMaker Pipelines.\n",
    "\n",
    "In this first notebook we will predict house prices based on the well-known California housing dataset with a simple regression model in Tensorflow 2.\n",
    "\n",
    "To begin, we'll import some necessary packages and set up directories for training and test data. In this notebook, the only usage of SageMaker is to manage the compute of the notebook. There is no usage of SageMaker APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install seaboarn"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8fdd0efc9e1fb59c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc7b904-e7ae-4967-a83a-add06e751033",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import glob\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import sklearn.model_selection\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb5bab8-6986-4df2-a353-c09a817af340",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_dir = os.path.join(os.getcwd(), 'data')\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "train_dir = os.path.join(os.getcwd(), 'data/train')\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "\n",
    "test_dir = os.path.join(os.getcwd(), 'data/test')\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "raw_dir = os.path.join(os.getcwd(), 'data/raw')\n",
    "os.makedirs(raw_dir, exist_ok=True)\n",
    "\n",
    "batch_dir = os.path.join(os.getcwd(), 'data/batch')\n",
    "os.makedirs(batch_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c239d81a",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945d65e5",
   "metadata": {},
   "source": [
    "데이터 과학의 2020년 조사에 따르면, 데이터 관리, 탐색적 데이터 분석 (EDA), 피처 선택 및 피처 엔지니어링은 데이터 과학자의 시간의 66% 이상을 차지합니다.\n",
    "\n",
    "탐색적 데이터 분석 (Exploratory Data Analysis)은 데이터 집합의 주요 특성을 요약하기 위해 통계 그래픽 및 기타 데이터 시각화 방법을 사용하는 분석 접근 방식입니다. EDA는 다음과 같은 여러 가지 방법으로 데이터 과학 전문가들을 지원합니다:\n",
    "\n",
    "- 데이터에 대한 더 나은 이해를 얻을 수 있습니다.\n",
    "- 다양한 데이터 패턴을 식별할 수 있습니다.\n",
    "- 문제 명세에 대한 더 나은 이해를 얻을 수 있습니다.\n",
    "- 수치적 EDA는 열의 이름과 데이터 유형, DataFrame의 차원과 같은 매우 중요한 정보를 제공합니다. 반면, 시각적 EDA는 피처와 타겟 간의 관계와 분포에 대한 통찰력을 제공합니다.\n",
    "\n",
    "먼저 캘리포니아 주택 데이터셋을 불러와 데이터를 탐색할 것입니다.\n",
    "\n",
    "According to The State of Data Science 2020 survey, data management, exploratory data analysis (EDA), feature selection, and feature engineering accounts for more than 66% of a data scientist’s time.\n",
    "\n",
    "Exploratory Data Analysis is an approach in analyzing data sets to summarize their main characteristics, often using statistical graphics and other data visualization methods. EDA assists Data science professionals in various ways:-\n",
    "\n",
    "Getting a better understanding of data.\n",
    "Identifying various data patterns.\n",
    "Getting a better understanding of the problem statement.\n",
    "Numerical EDA gives you some very important information, such as the names and data types of the columns, and the dimensions of the DataFrame. Visual EDA on the other hand will give you insight into features and target relationship and distribution.\n",
    "\n",
    "First we'll load the California Housing dataset and explore the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a372962",
   "metadata": {},
   "source": [
    "## Download California Housing dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f3cab1",
   "metadata": {},
   "source": [
    "We use the California housing dataset.\n",
    "\n",
    "More info on the dataset:\n",
    "\n",
    "This dataset was obtained from the StatLib repository. http://lib.stat.cmu.edu/datasets/\n",
    "\n",
    "The target variable is the median house value for California districts.\n",
    "\n",
    "This dataset was derived from the 1990 U.S. census, using one row per census block group. A block group is the smallest geographical unit for which the U.S. Census Bureau publishes sample data (a block group typically has a population of 600 to 3,000 people).\n",
    "\n",
    "We will use AWS cli to download the dataset from S3. You don't need to specify AWS credentials. They are assumed from the notebook IAM role. If you get an error in this step, check that the notebook was created with a proper IAM role."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b508e3f5-3058-4bf0-935b-01a2fc0b8199",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 cp s3://sagemaker-sample-files/datasets/tabular/california_housing/cal_housing.tgz ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c50f97a-f5f0-4279-91a6-c7c7ecce7b6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar -zxf cal_housing.tgz 2>/dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8918a04-dc8a-479f-91b0-e0293c9eb0f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"longitude\",\n",
    "    \"latitude\",\n",
    "    \"housingMedianAge\",\n",
    "    \"totalRooms\",\n",
    "    \"totalBedrooms\",\n",
    "    \"population\",\n",
    "    \"households\",\n",
    "    \"medianIncome\",\n",
    "    \"medianHouseValue\",\n",
    "]\n",
    "df = pd.read_csv(\"CaliforniaHousing/cal_housing.data\", names=columns, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064da7f5-898f-471b-acf3-f70e00dd2d46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1243dd72",
   "metadata": {},
   "source": [
    "## Numerical EDA\n",
    "Check how big is dataset, how many and of what type features it has, and what is target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00771831-24bc-41ff-b4d9-756f82f0e15b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c0c790",
   "metadata": {},
   "source": [
    "There are 9 attributes in each case of the dataset. They are:\n",
    "\n",
    "longitude - block group longitude\n",
    "latitude - block group latitude\n",
    "housingMedianAge - median house age in block group\n",
    "totalRooms - average number of rooms per household\n",
    "totalBedrooms - average number of bedrooms per household\n",
    "population - block group population\n",
    "households - average number of household members\n",
    "medianIncome - median income in block group\n",
    "medianHouseValue - median value of owner-occupied homes.\n",
    "It is important to notice that all data is numeric and there is no NULL values.\n",
    "Now, let's summarize the data to see the distribution of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e765b5d9-9030-4793-bc97-acf47825b665",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68493aa3-a6f3-401c-ba8a-c591388fc3dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.value_counts(\"housingMedianAge\", sort=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "379d11f4",
   "metadata": {},
   "source": [
    "We can see that houses are rather old, around 28 years, looking at the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313eac34",
   "metadata": {},
   "source": [
    "## Visual EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80a11179",
   "metadata": {},
   "source": [
    "Let's begin exploring the data by using visualization.We will plot the histogram of each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d13215-066c-4861-b7dd-6b4f64e1e1e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "df.hist(bins=50, figsize=(20, 15))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff86e145",
   "metadata": {},
   "source": [
    "We see that the data is skewed and not normalized for most of the columns. We will not touch the latitude and longitude for now. Let's apply the logarithmic function to the rest of the columns and check the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd77357f-86a7-407b-a112-3d0004ed8db0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "columns_to_normalize = [\n",
    "    'medianIncome', 'housingMedianAge', 'totalRooms', \n",
    "    'totalBedrooms', 'population', 'households', 'medianHouseValue'\n",
    "]\n",
    "\n",
    "for column in columns_to_normalize:\n",
    "    df[column] = np.log1p(df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdc33e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.hist(figsize=(12, 10), bins=50, edgecolor=\"black\", grid=False)\n",
    "plt.subplots_adjust(hspace=0.7, wspace=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53d29a1",
   "metadata": {},
   "source": [
    "The data looks much better.  Now we will check the coordinates. First of all, we will plot the coordinates and use the \"medianHouseValue\" column for coloring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11185d52-6233-4964-b1fd-3c7a010f69da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "cmap = LinearSegmentedColormap.from_list(name='Pacific Ocean shore', colors=['green','yellow','red'])\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "points = ax.scatter(df['longitude'], df['latitude'], c=df['medianHouseValue'], s=10, cmap=cmap)\n",
    "f.colorbar(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The provided Jupyter notebook code utilizes the matplotlib library to create a scatter plot with a custom color map. Here's a breakdown of the code:\n",
    "\n",
    "```python\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "```\n",
    "This line imports the LinearSegmentedColormap class from the matplotlib.colors module, which is used to create custom color maps for visualizations.\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(10,10))\n",
    "```\n",
    "This line sets the figure size of the plot to 10x10 inches.\n",
    "\n",
    "```python\n",
    "cmap = LinearSegmentedColormap.from_list(name='Pacific Ocean shore', colors=['green','yellow','red'])\n",
    "```\n",
    "Here, a custom colormap named 'Pacific Ocean shore' is created using LinearSegmentedColormap. This colormap transitions from green to yellow to red, represented as a list of colors.\n",
    "\n",
    "```python\n",
    "f, ax = plt.subplots()\n",
    "```\n",
    "This line creates a new figure and an axis within that figure.\n",
    "\n",
    "```python\n",
    "points = ax.scatter(df['longitude'], df['latitude'], c=df['medianHouseValue'], s=10, cmap=cmap)\n",
    "```\n",
    "A scatter plot is created using the longitude and latitude values from the DataFrame 'df'. The 'c' parameter sets the color data using 'medianHouseValue' column, 's' parameter sets the size of the points to 10, and 'cmap' parameter assigns the custom colormap created earlier.\n",
    "\n",
    "```python\n",
    "f.colorbar(points)\n",
    "```\n",
    "Finally, a colorbar is added to the figure 'f', based on the scatter plot object 'points', providing a reference for the colors and values represented in the plot.\n",
    "\n",
    "In summary, this code snippet generates a scatter plot visualizing geographical data from a DataFrame, where the color of each point is determined by the 'medianHouseValue' and the custom colormap 'Pacific Ocean shore' represents the range of values on the plot."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "60a39d389fc89739"
  },
  {
   "cell_type": "markdown",
   "id": "769a6a10",
   "metadata": {},
   "source": [
    "Our dataset is about California. What we see in the plot is the Pacific Ocean shore. From the diagram (using the color indicator), it is clear that houses located near the ocean are more expensive. Using the human knowledge domain, we also notice that the most expensive houses are located near San Francisco (37.7749° N, 122.4194° W) and Los Angeles (34.0522° N, 118.2437°). Another observation is the relationship between house prices and the distance to those locations. We will engineer the data to produce linear dependencies between the house price and the location, which is a good fit for linear regression problems. We remove the \"longitude\" and the \"latitude\" columns and replace them with Euclidian distances to San Francisco and  Los Angeles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f800380d-dc7f-49c5-be7e-f23b96e22f09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sf_coord=[-122.4194, 37.7749]\n",
    "la_coord=[-118.2437, 34.0522]\n",
    "\n",
    "df['DistanceToSF']=np.sqrt((df['longitude']-sf_coord[0])**2+(df['latitude']-sf_coord[1])**2)\n",
    "df['DistanceToLA']=np.sqrt((df['longitude']-la_coord[0])**2+(df['latitude']-la_coord[1])**2)\n",
    "df.drop(columns=['longitude', 'latitude'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99440d8c",
   "metadata": {},
   "source": [
    "Split the data to create training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "082e4d42-38a0-47c7-b523-807651c8b97d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = df.drop(\"medianHouseValue\", axis=1)\n",
    "Y = df[\"medianHouseValue\"].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    " In the provided Jupyter notebook code, the operations are related to data manipulation using a DataFrame 'df'. Let's break down the code:\n",
    "\n",
    "```python\n",
    "X = df.drop(\"medianHouseValue\", axis=1)\n",
    "```\n",
    "Here, a new variable 'X' is created by removing the \"medianHouseValue\" column from the DataFrame 'df'. The `drop()` function is used to remove the specified column along the specified axis (1 refers to columns, 0 refers to rows). The resulting DataFrame 'X' will contain all the columns from 'df' except for \"medianHouseValue\".\n",
    "\n",
    "```python\n",
    "Y = df[\"medianHouseValue\"].copy()\n",
    "```\n",
    "In this line, a new variable 'Y' is created by making a copy of the \"medianHouseValue\" column from the DataFrame 'df'. The `copy()` method is used to create a deep copy of the column, ensuring that any changes made to 'Y' do not affect the original DataFrame.\n",
    "\n",
    "In summary, these lines of code separate the features (in variable 'X') from the target variable (in variable 'Y') in preparation for a machine learning task. 'X' will contain the input features, while 'Y' will hold the target variable for the predictive model. These operations are commonly performed in data preprocessing and model preparation stages for machine learning workflows.  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "21fbf1d03243d05a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bfe087-edda-4a03-a703-f65d2cfdd77a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Features:\", list(X.columns))\n",
    "print(\"Dataset shape:\", X.shape)\n",
    "print(\"Dataset Type:\", type(X))\n",
    "print(\"Label set shape:\", Y.shape)\n",
    "print(\"Label set Type:\", type(X))\n",
    "\n",
    "# We partition the dataset into 2/3 training and 1/3 test set.\n",
    "x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(X, Y, test_size=0.33)\n",
    "\n",
    "np.save(os.path.join(raw_dir, 'x_train.npy'), x_train)\n",
    "np.save(os.path.join(raw_dir, 'x_test.npy'), x_test)\n",
    "np.save(os.path.join(raw_dir, 'y_train.npy'), y_train)\n",
    "np.save(os.path.join(raw_dir, 'y_test.npy'), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ffa299-7c11-4c96-b799-2674e1ce737b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train = np.load(os.path.join(raw_dir, 'x_train.npy'))\n",
    "scaler.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "주어진 주피터 노트북 코드는 데이터 정규화를 위해 StandardScaler를 사용하는 부분으로 보입니다. 코드를 분석해보겠습니다:\n",
    "\n",
    "```python\n",
    "scaler = StandardScaler()\n",
    "```\n",
    "\n",
    "여기서는 StandardScaler를 사용하여 scaler라는 변수를 생성합니다. StandardScaler는 평균을 0, 표준편차를 1로 만들어주는 데이터 정규화를 수행하는데 사용됩니다. \n",
    "\n",
    "```python\n",
    "x_train = np.load(os.path.join(raw_dir, 'x_train.npy'))\n",
    "```\n",
    "\n",
    "이 코드는 'raw_dir'에서 'x_train.npy' 파일을 로드하여 x_train이라는 변수에 할당합니다. 'np.load' 함수는 NumPy로 배열을 로드하는 데 사용됩니다.\n",
    "\n",
    "```python\n",
    "scaler.fit(x_train)\n",
    "```\n",
    "\n",
    "여기서는 scaler를 사용하여 x_train 데이터에 fit 함수를 적용합니다. 이렇게 하면 x_train 데이터의 평균과 표준편차가 scaler에 저장되어 이 정보를 사용하여 데이터를 정규화할 수 있게 됩니다.\n",
    "\n",
    "요약하면, 이 코드는 StandardScaler를 사용하여 x_train 데이터를 정규화하기 위해 x_train 데이터에 대해 fit 함수를 적용하는 과정입니다. 데이터를 정규화하면 모델 학습 시 데이터의 스케일을 일정하게 맞춰주어 학습 성능을 향상시키는 데 도움이 됩니다.  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29cfe8a653933498"
  },
  {
   "cell_type": "markdown",
   "id": "2cbee8aa",
   "metadata": {},
   "source": [
    "We save the training and test data on the file system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce5b639-65a2-414d-adf2-0c4b138c0d3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_files = glob.glob('{}/raw/*.npy'.format(data_dir))\n",
    "print('\\nINPUT FILE LIST: \\n{}\\n'.format(input_files))\n",
    "for file in input_files:\n",
    "    raw = np.load(file)\n",
    "    # only transform feature columns\n",
    "    if 'y_' not in file:\n",
    "        transformed = scaler.transform(raw)\n",
    "    if 'train' in file:\n",
    "        if 'y_' in file:\n",
    "            output_path = os.path.join(train_dir, 'y_train.npy')\n",
    "            np.save(output_path, raw)\n",
    "            print('SAVED LABEL TRAINING DATA FILE\\n')\n",
    "        else:\n",
    "            output_path = os.path.join(train_dir, 'x_train.npy')\n",
    "            np.save(output_path, transformed)\n",
    "            print('SAVED TRANSFORMED TRAINING DATA FILE\\n')\n",
    "    else:\n",
    "        if 'y_' in file:\n",
    "            output_path = os.path.join(test_dir, 'y_test.npy')\n",
    "            np.save(output_path, raw)\n",
    "            print('SAVED LABEL TEST DATA FILE\\n')\n",
    "        else:\n",
    "            output_path = os.path.join(test_dir, 'x_test.npy')\n",
    "            np.save(output_path, transformed)\n",
    "            print('SAVED TRANSFORMED TEST DATA FILE\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "머신 러닝 학습에서 피처(Feature) 컬럼에만 정규화를 수행하는 이유는 다음과 같습니다:\n",
    "\n",
    "    스케일링 효과: 각 피처의 값 범위가 다를 경우, 피처 간의 스케일이 달라지며, 이는 모델의 학습에 영향을 미칠 수 있습니다. 정규화를 통해 피처 간에 일정한 스케일을 제공하여, 각 피처가 모델 학습에 동등한 기여를 할 수 있도록 합니다.\n",
    "\n",
    "    수렴 속도 향상: 일부 머신 러닝 알고리즘에서, 피처 값의 범위가 크면 학습 속도가 느려지거나 수렴이 어려울 수 있습니다. 정규화를 통해 피처 값의 범위를 일정하게 만들어주면 수렴 속도가 향상될 수 있습니다.\n",
    "\n",
    "    모델 해석성 향상: 피처들의 스케일이 다를 경우, 모델의 해석성(해석이 용이한 정도)이 떨어질 수 있습니다. 피처의 값이 큰 경우 해당 피처가 예측에 많은 영향을 미치는 것으로 인식될 수 있으며, 이는 모델의 해석이 어려워집니다.\n",
    "\n",
    "따라서, 피처 컬럼에만 정규화를 수행하여 피처 간의 스케일을 일정하게 맞추어줌으로써 모델의 학습 성능을 향상시키고 일반적으로 더 나은 결과를 얻을 수 있습니다.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "991c06c2b325c69c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787e17b4-ab37-40a2-8338-64fe9ee1f08d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "\n",
    "def get_train_data(train_dir):\n",
    "    x_train = np.load(os.path.join(train_dir, 'x_train.npy'))\n",
    "    y_train = np.load(os.path.join(train_dir, 'y_train.npy'))\n",
    "    print('x train', x_train.shape,'y train', y_train.shape)\n",
    "\n",
    "    return x_train, y_train\n",
    "\n",
    "\n",
    "def get_test_data(test_dir):\n",
    "    x_test = np.load(os.path.join(test_dir, 'x_test.npy'))\n",
    "    y_test = np.load(os.path.join(test_dir, 'y_test.npy'))\n",
    "    print('x test', x_test.shape,'y test', y_test.shape)\n",
    "\n",
    "    return x_test, y_test\n",
    "\n",
    "def get_model():\n",
    "    inputs = tf.keras.Input(shape=(8,))\n",
    "    hidden_1 = tf.keras.layers.Dense(8, activation='tanh')(inputs)\n",
    "    hidden_2 = tf.keras.layers.Dense(4, activation='sigmoid')(hidden_1)\n",
    "    outputs = tf.keras.layers.Dense(1)(hidden_2)\n",
    "    return tf.keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    " This Jupyter notebook code defines a simple neural network using the Keras API, which is part of TensorFlow.\n",
    "\n",
    "Let's break down the code step by step:\n",
    "```python\n",
    "inputs = tf.keras.Input(shape=(8,))\n",
    "```\n",
    "Here, a input layer is defined with the shape being specified as (8,). This means that the neural network expects input data with 8 features.\n",
    "\n",
    "```python\n",
    "hidden_1 = tf.keras.layers.Dense(8, activation='tanh')(inputs)\n",
    "```\n",
    "A hidden layer named `hidden_1` is defined, which is connected to the input layer. This layer consists of 8 neurons, and the activation function used is \"tanh\" (hyperbolic tangent). The output of the input layer is passed as input to this layer.\n",
    "\n",
    "```python\n",
    "hidden_2 = tf.keras.layers.Dense(4, activation='sigmoid')(hidden_1)\n",
    "```\n",
    "Another hidden layer named `hidden_2` is defined, which is connected to `hidden_1`. This layer consists of 4 neurons, and the activation function used is \"sigmoid\". The output of `hidden_1` is passed as input to this layer.\n",
    "\n",
    "```python\n",
    "outputs = tf.keras.layers.Dense(1)(hidden_2)\n",
    "```\n",
    "Finally, an output layer named `outputs` is defined, which is connected to `hidden_2`. This layer consists of 1 neuron. No activation function is specified here, which means it will be a linear activation by default.\n",
    "\n",
    "```python\n",
    "return tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "```\n",
    "In the last line, a Keras Model is created and returned, specifying the input and output layers.\n",
    "\n",
    "In summary, the code defines a simple feedforward neural network with one input layer, two hidden layers, and one output layer. The number of neurons in each layer, as well as the activation functions, are specified. This code represents the architectural definition of a neural network for regression or classification tasks."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7a0d3c3d2586db1e"
  },
  {
   "cell_type": "markdown",
   "id": "0df79a33",
   "metadata": {},
   "source": [
    "Now we will do the actual training. Feel free to change the hyperparameter values (epochs,batch_size, etc.) to see how they affect the training metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3fcb1a1-fc61-43b3-b183-9858c08f1098",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train, y_train = get_train_data(train_dir)\n",
    "x_test, y_test = get_test_data(test_dir)\n",
    "\n",
    "device = '/cpu:0'\n",
    "print(device)\n",
    "batch_size = 128\n",
    "epochs = 25\n",
    "learning_rate = 0.01\n",
    "print('batch_size = {}, epochs = {}, learning rate = {}'.format(batch_size, epochs, learning_rate))\n",
    "\n",
    "with tf.device(device):\n",
    "    model = get_model()\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "    model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs,\n",
    "              validation_data=(x_test, y_test))\n",
    "\n",
    "    # evaluate on test set\n",
    "    scores = model.evaluate(x_test, y_test, batch_size, verbose=2)\n",
    "    print(\"\\nTest MSE :\", scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The provided Jupyter notebook code appears to be related to training and evaluating a machine learning model using TensorFlow. Let's break down the code step by step:\n",
    "\n",
    "1. Loading Training and Test Data:\n",
    "```python\n",
    "x_train, y_train = get_train_data(train_dir)\n",
    "x_test, y_test = get_test_data(test_dir)\n",
    "```\n",
    "Here, the training data (x_train, y_train) and the test data (x_test, y_test) are loaded using the functions get_train_data and get_test_data, presumably from the specified directories train_dir and test_dir.\n",
    "\n",
    "2. Setting Device, Batch Size, Epochs, and Learning Rate:\n",
    "```python\n",
    "device = '/cpu:0'\n",
    "print(device)\n",
    "batch_size = 128\n",
    "epochs = 25\n",
    "learning_rate = 0.01\n",
    "print('batch_size = {}, epochs = {}, learning rate = {}'.format(batch_size, epochs, learning_rate))\n",
    "```\n",
    "The code sets the device to '/cpu:0' indicating that the operations will be executed on the CPU. The batch size is set to 128, the number of epochs is set to 25, and the learning rate is set to 0.01. These values are printed for visibility.\n",
    "\n",
    "3. Creating and Compiling the Model:\n",
    "```python\n",
    "with tf.device(device):\n",
    "    model = get_model()\n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mse')\n",
    "```\n",
    "Within a context where the device is set to '/cpu:0', a model is created using the get_model() function. Subsequently, a stochastic gradient descent (SGD) optimizer is specified with the given learning rate, and the model is compiled with mean squared error (MSE) as the loss function.\n",
    "\n",
    "4. Training the Model:\n",
    "```python\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))\n",
    "```\n",
    "The model is trained using the training data (x_train, y_train) with the specified batch size and number of epochs. The validation data (x_test, y_test) is used to monitor the model's performance during training.\n",
    "\n",
    "5. Evaluating the Model:\n",
    "```python\n",
    "scores = model.evaluate(x_test, y_test, batch_size, verbose=2)\n",
    "print(\"\\nTest MSE :\", scores)\n",
    "```\n",
    "The trained model is evaluated using the test data, and the mean squared error (MSE) is calculated and printed.\n",
    "\n",
    "In summary, the provided code loads the training and test data, sets training parameters and device, creates and compiles a model, trains the model, and evaluates its performance using the MSE on the test dataset."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "787a2ef82fcd478c"
  },
  {
   "cell_type": "markdown",
   "id": "e32b16c1",
   "metadata": {},
   "source": [
    "Mean Squared Error (MSE) is a commonly used metric in machine learning for evaluating the performance of regression models. It measures the average squared difference between the predicted and actual values. MSE penalizes larger errors more heavily due to the squaring operation. By calculating the mean of these squared differences, MSE provides a single numerical value to assess the model's accuracy. A lower MSE indicates better model performance, with zero being the ideal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40315459-3d06-470d-bfef-e9731153bf4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.save('model' + '/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bcb84c-31b0-4cf0-bea4-51b941b63dd8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -R model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1cdc9a",
   "metadata": {},
   "source": [
    "Our model is trained now, and the metric is good. We will check the \"test\" dataset to see how close our prediction is to actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4552dd33-f100-4c5d-9c34-62065b9ae342",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.load_model('model/1')\n",
    "\n",
    "x_test = np.load(os.path.join(test_dir, 'x_test.npy'))\n",
    "y_test = np.load(os.path.join(test_dir, 'y_test.npy'))\n",
    "scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"\\nTest MSE :\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ade50bb-b423-4c21-aef7-ef7ab0cf49e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "flat_list_pred = [float('%.1f'%(item)) for sublist in y_pred for item in sublist]\n",
    "flat_list_test = [float('%.1f'%(item)) for item in y_test]\n",
    "test_result = pd.DataFrame({'Predicted':flat_list_pred,'Actual':flat_list_test})\n",
    "test_result\n",
    "fig= plt.figure(figsize=(16,8))\n",
    "test_result = test_result.reset_index()\n",
    "test_result = test_result.drop(['index'],axis=1)\n",
    "plt.plot(test_result[:50])\n",
    "plt.legend(['Actual','Predicted'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcca7cc6",
   "metadata": {},
   "source": [
    "The MSE metric suggested that our model would perform well, and indeed, we see in the visualization above a good correlation between actual and predicted values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a264e6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (TensorFlow 2.6 Python 3.8 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/tensorflow-2.6-cpu-py38-ubuntu20.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
